{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf35f13f",
   "metadata": {},
   "source": [
    "# EXERCISE\n",
    "In this exercise you will write the key components of both a Q-learning and SARSA agent and train them to solve the frozen lake environment: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de835ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\"\")))\n",
    "from utils_nt import plot_value, plot_returns, plot_explore\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c8a14",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Try and understand roughly how this code works:\n",
    "* Where is the agents policy queried to get the next action?\n",
    "* Where is the action applied to the environment?\n",
    "* Where is the state, action, reward, next_state, next_action constructed?\n",
    "* Which block of code runs an entire episode?\n",
    "* Find where the different agents are updated, why is monte-carlo agent updated in a different place in the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env,  n_episodes = 10001, explore_rate = 0.05, monte_carlo = False, display=False):\n",
    "\n",
    "    returns = []\n",
    "    \n",
    "    for episode in range(0,n_episodes): # for each episode\n",
    "\n",
    "        e_return = 0 # sum the reward we get this episode\n",
    "        e_transitions = [] # memory of all tranisitions seen in this episode\n",
    "        done = False # has the episode finished?\n",
    "        \n",
    "        state = env.reset() # reset env to initial state\n",
    "        \n",
    "        action = agent.epsilon_greedy_policy(state, explore_rate) # initialise the agent\n",
    "\n",
    "        if display: # display the current agent-evironment state\n",
    "            env.render(mode='human')\n",
    "        \n",
    "        \n",
    "        while not done: # run the episode until a terminal state reached\n",
    "\n",
    "            next_state, reward, done, info = env.step(action) # take an action and get the resulting state from the env\n",
    "            next_action = agent.epsilon_greedy_policy(next_state, explore_rate) # get the next action to apply from agent's policy\n",
    "            transition = (state, action, reward, next_state, next_action, done) # create the SARSA transition\n",
    "            e_transitions.append(transition) # add to the memory\n",
    "\n",
    "            if not monte_carlo: # update the Q function of temporal difference agents\n",
    "                agent.update_Q(transition)\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "            e_return += reward\n",
    "            \n",
    "            if display: # display the current agent-evironment state\n",
    "                env.render(mode='human')\n",
    "\n",
    "\n",
    "        returns.append(e_return)\n",
    "        \n",
    "        if monte_carlo: # update the q function of a monte carlo agent\n",
    "            agent.update_Q(e_transitions)\n",
    "\n",
    "        if episode % 1000 == 0: # print results of current episode\n",
    "            print('episode:', episode, ', explore_rate:', explore_rate, ', return:', e_return)\n",
    "            \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4fe46",
   "metadata": {},
   "source": [
    "# 1. Fill in the Q learning agent's policy and update function \n",
    "You will need to:\n",
    "- Initialise the Q_func. Create a numpy array full of zeros which will store the values for all the state-action pairs. \n",
    "- Write code for the epsilon greedy policy. Use the np.random.random function to determine whether the agent should take a random exploratory action with the explore_rate. If an exploratory action is taken, choose an action randomly from the set of available actions using the np.random.choice function. If we instead take a exploitative action use the np.argmax function to take the action with the highest value given the current state. \n",
    "- Write code for the update_Q function. This should increment the Q_func for the current state-action pair according to the Q-learning update rule.\n",
    "\n",
    "If you get stuck the code examples in the slides will help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97353af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_agent():\n",
    "\n",
    "    def __init__(self, n_states, n_actions, gamma = 0.95, alpha = 0.01):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "       \n",
    "        # ******* Your code here *******\n",
    "        self.Q_func = # initialise the Q table\n",
    "        \n",
    "\n",
    "\n",
    "    def epsilon_greedy_policy(self, state, explore_rate):\n",
    "        '''\n",
    "        chooses an action based on the agents value function and the current explore rate\n",
    "        :param state: the current state given by the environment\n",
    "        :param explore_rate: the chance of taking a random action\n",
    "        :return: the action to be taken\n",
    "        '''\n",
    "\n",
    "\n",
    "        #####\n",
    "        # ******* Your code here *******\n",
    "        #\n",
    "        ####\n",
    "    \n",
    "    def update_Q(self,transition):\n",
    "        '''\n",
    "        updates the agents value function based on the experience in transition\n",
    "        :param transition:\n",
    "        :return:\n",
    "        '''\n",
    "        \n",
    "        state, action, reward, next_state, next_action, done = transition\n",
    "        \n",
    "        #####\n",
    "        # ******* Your code here ******* \n",
    "        #\n",
    "        ####\n",
    "        \n",
    "    def get_explore_rate(self, episode, decay, min_r = 0, max_r = 1):\n",
    "        '''\n",
    "        Calculates the logarithmically decreasing explore or learning rate\n",
    "\n",
    "        Parameters:\n",
    "            episode: the current episode\n",
    "            min_r: the minimum possible step size\n",
    "            max_r: maximum step size\n",
    "            decay: controls the rate of decay of the step size\n",
    "        Returns:\n",
    "            step_size: the Q-learning step size\n",
    "        '''\n",
    "\n",
    "        rate = max(min_r, min(max_r, 1.0 - math.log10((episode + 1) / decay)))\n",
    "\n",
    "        return rate\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac23875",
   "metadata": {},
   "source": [
    "## 2. Train the Q learning agent\n",
    "Train and test the Q_learning agent using the code below\n",
    "\n",
    "Examine the output plots:\n",
    "- Do the output plots indicate that you agent has been trained sucessfully\n",
    "- Has it found the optimal policy? If not try and debug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ab29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent \n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "env.render_mode = 'human'\n",
    "\n",
    "# ******* Your code here *******\n",
    "# Fill in these variables, look at the OpenAIGym documentation to find the correct values \n",
    "n_states = \n",
    "n_actions = \n",
    "grid_shape = \n",
    "\n",
    "p_map = {0:'L', 1:'D', 2:'R', 3:'U'}\n",
    "\n",
    "agent = Q_agent(n_states, n_actions)\n",
    "n_episodes = 10001\n",
    "\n",
    "print('TRAINING')\n",
    "returns = train_agent(agent, env, n_episodes = n_episodes, explore_rate = 0.05) # train the agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41984702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test episode\n",
    "print()\n",
    "print('TEST EPISODE')\n",
    "r = train_agent(agent, env, n_episodes=1, explore_rate=0, display=True) # run a test episode to get the learned behaviour with no exploring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value function and policy\n",
    "plot_value(agent.Q_func, grid_shape, p_map) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e295daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot returns and explore rate \n",
    "plot_returns(returns)\n",
    "plot_explore(agent.get_explore_rate, n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf315d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7614e",
   "metadata": {},
   "source": [
    "# 3. Now create a  SARSA_agent. \n",
    "- Hint: the initialisation and the policy functions will be the same as the Q_agent, just change the update_Q function to increment the Q_func for the current state-action pair according to the SARSA update rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA_agent():\n",
    "\n",
    "    def __init__(self, n_states, n_actions, gamma = 0.95, alpha = 0.01):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # THIS WILL BE THE SAME AS THE Q_agent\n",
    "        # ******* Your code here ******* \n",
    "        self.Q_func = # initialise the Q table\n",
    "        \n",
    "\n",
    "\n",
    "    def epsilon_greedy_policy(self, state, explore_rate):\n",
    "        '''\n",
    "        chooses an action based on the agents value function and the current explore rate\n",
    "        :param state: the current state given by the environment\n",
    "        :param explore_rate: the chance of taking a random action\n",
    "        :return: the action to be taken\n",
    "        '''\n",
    "\n",
    "\n",
    "        #####  THIS WILL BE THE SAME AS THE Q_agent\n",
    "        # ******* Your code here *******\n",
    "        #\n",
    "        ####\n",
    "    \n",
    "    def update_Q(self,transition):\n",
    "        '''\n",
    "        updates the agents value function based on the experience in transition\n",
    "        :param transition:\n",
    "        :return:\n",
    "        '''\n",
    "        state, action, reward, next_state, next_action, done = transition\n",
    "        #####\n",
    "        # ******* Your code here ******* \n",
    "        #\n",
    "        ####\n",
    "        \n",
    "    def get_explore_rate(self, episode, decay, min_r = 0, max_r = 1):\n",
    "        '''\n",
    "        Calculates the logarithmically decreasing explore or learning rate\n",
    "\n",
    "        Parameters:\n",
    "            episode: the current episode\n",
    "            min_r: the minimum possible step size\n",
    "            max_r: maximum step size\n",
    "            decay: controls the rate of decay of the step size\n",
    "        Returns:\n",
    "            step_size: the Q-learning step size\n",
    "        '''\n",
    "\n",
    "        rate = max(min_r, min(max_r, 1.0 - math.log10((episode + 1) / decay)))\n",
    "\n",
    "        return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcd40f",
   "metadata": {},
   "source": [
    "## 4. Train the SARSA learning agent\n",
    "Train and test the SARSA_learning agent using the code below\n",
    "\n",
    "Examine the output plots:\n",
    "- Do the output plots indicate that you agent has been trained sucessfully?\n",
    "- Has it found the optimal policy? If not try and debug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6dff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent \n",
    "agent = SARSA_agent(n_states, n_actions)\n",
    "n_episodes = 10001\n",
    "\n",
    "print('TRAINING')\n",
    "returns = train_agent(agent, env, n_episodes = n_episodes, explore_rate = 0.05) # train the agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test episode\n",
    "print()\n",
    "print('TEST EPISODE')\n",
    "r = train_agent(agent, env, n_episodes=1, explore_rate=0, display=True) # run a test episode to get the learned behaviour with no exploring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ee170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value function and policy\n",
    "plot_value(agent.Q_func, grid_shape, p_map) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ca53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot returns and explore rate \n",
    "plot_returns(returns)\n",
    "plot_explore(agent.get_explore_rate, n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e358fc7",
   "metadata": {},
   "source": [
    "### Extension: \n",
    "\n",
    "- Try and solve one of the other toy text environments from OpenAIgym (https://www.gymlibrary.dev/environments/toy_text/) using Q learning or SARSA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfaaad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
